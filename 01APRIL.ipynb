{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    return np.random.randn(input_size, hidden_size), np.random.randn(hidden_size, output_size)\n",
    "\n",
    "def forward_propagation(inputs, weights_input_hidden, weights_hidden_output, activation_fn):\n",
    "    hidden_layer_output = activation_fn(np.dot(inputs, weights_input_hidden))\n",
    "    output_layer_output = sigmoid(np.dot(hidden_layer_output, weights_hidden_output))\n",
    "    return hidden_layer_output, output_layer_output\n",
    "\n",
    "def backward_propagation(inputs, targets, hidden_layer_output, output_layer_output,\n",
    "                         weights_input_hidden, weights_hidden_output, learning_rate, activation_derivative):\n",
    "    output_error = targets - output_layer_output\n",
    "    output_delta = output_error * activation_derivative(output_layer_output)\n",
    "    hidden_layer_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_layer_delta = hidden_layer_error * activation_derivative(hidden_layer_output)\n",
    "\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden += inputs.T.dot(hidden_layer_delta) * learning_rate\n",
    "\n",
    "def train_neural_network(inputs, targets, hidden_size, output_size, learning_rate, epochs, activation_fn, activation_derivative):\n",
    "    weights_input_hidden, weights_hidden_output = initialize_weights(inputs.shape[1], hidden_size, output_size)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer_output, output_layer_output = forward_propagation(inputs, weights_input_hidden, weights_hidden_output, activation_fn)\n",
    "        backward_propagation(inputs, targets, hidden_layer_output, output_layer_output,\n",
    "                             weights_input_hidden, weights_hidden_output, learning_rate, activation_derivative)\n",
    "\n",
    "        loss = np.mean((targets - output_layer_output) ** 2)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return weights_input_hidden, weights_hidden_output, loss_history\n",
    "\n",
    "def plot_loss_curves(*loss_histories, labels, title='Training Loss for Different Activation Functions'):\n",
    "    for loss_history, label in zip(loss_histories, labels):\n",
    "        plt.plot(loss_history, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "targets = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "sigmoid_params = train_neural_network(inputs, targets, hidden_size, output_size, learning_rate, epochs, sigmoid, sigmoid_derivative)\n",
    "tanh_params = train_neural_network(inputs, targets, hidden_size, output_size, learning_rate, epochs, tanh, tanh_derivative)\n",
    "relu_params = train_neural_network(inputs, targets, hidden_size, output_size, learning_rate, epochs, relu, relu_derivative)\n",
    "\n",
    "plot_loss_curves(sigmoid_params[2], tanh_params[2], relu_params[2], labels=['Sigmoid', 'Tanh', 'ReLU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
